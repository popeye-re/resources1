Representation of a Text Document in Vector Space Model and Computing Similarity 
between two documents. 
AIM: 
To compute the similarity between two text documents using TF-IDF vectorization and Cosine 
Similarity in Python. 
DESCRIPTION: 
Cosine Similarity – Definition: 
Cosine Similarity is a metric used to measure the similarity between two non-zero vectors by 
calculating the cosine of the angle between them. 
In the context of text documents, it compares two documents represented as TF-IDF vectors and 
returns a value between 0 and 1: 
 1 indicates identical documents (high similarity). 
 0 indicates completely different documents. 
It is widely used in text mining, information retrieval, and natural language processing to 
compare the semantic similarity between documents. 
Vector Space Model – Definition: 
The Vector Space Model is a mathematical model used to represent text documents as vectors in a 
multi-dimensional space. 
Each unique term in the entire corpus forms a separate dimension. The values in the vector are 
typically term weights, such as TF-IDF scores, which indicate the importance of a word in a 
document. 
In this model: 
 Documents are treated as vectors. 
 Similarity can be computed using mathematical techniques like cosine similarity. 
This model enables efficient document comparison, ranking, and clustering in text-based 
applications.

pdf see


Interpretation of Result: 
 1 → Documents are exactly the same in direction (very similar). 
 0 → Documents are completely different (orthogonal vectors). 
 Closer to 1 → More similar.


doc1 = "Machine learning is amazing and fun" 
doc2 = "Deep learning and machine learning are parts of artificial intelligence" 
documents = [doc1, doc2] 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity 

# Create TF-IDF vectorizer 

vectorizer = TfidfVectorizer() 

# Fit and transform the documents 

tfidf_matrix = vectorizer.fit_transform(documents) 

# Display the feature names (terms) 

print("Vocabulary:", vectorizer.get_feature_names_out()) 

# Display TF-IDF matrix 

print("TF-IDF Matrix:\n", tfidf_matrix.toarray()) 

# Compute cosine similarity between the two documents 

similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2]) 
print("Cosine Similarity between doc1 and doc2:", similarity[0][0])

OUTPUT:
Vocabulary: ['amazing' 'and' 'are' 'artificial' 'deep' 'fun' 'intelligence' 'is'
 'learning' 'machine' 'of' 'parts']
TF-IDF Matrix:
 [[0.47042643 0.33471228 0.         0.         0.         0.47042643
  0.         0.47042643 0.33471228 0.33471228 0.         0.        ]
 [0.         0.23667732 0.33264172 0.33264172 0.33264172 0.
  0.33264172 0.         0.47335464 0.23667732 0.33264172 0.33264172]]
Cosine Similarity between doc1 and doc2: 0.3168752218840048


WEK-8:
Implement Matrix Decomposition and LSI for a standard dataset.  
AIM:  
To implement matrix decomposition and LSI for a standard dataset. 
DESCRIPTION: 
Matrix Decomposition 
 
Matrix decomposition (or matrix factorization) is a mathematical technique used to break down a 
large matrix into smaller, simpler matrices. 
In text mining, we typically start with a document-term matrix (DTM) or TF-IDF matrix 
where: 
o Rows = documents 
o Columns = terms (words) 
o Values = frequency or importance of terms in documents 
Latent Semantic Indexing (LSI) 
Latent Semantic Indexing (also called Latent Semantic Analysis, LSA) is a technique in information 
retrieval and natural language processing that uses SVD on the term-document matrix. 
Idea behind LSI 
 Natural language has synonyms (different words with similar meaning) and polysemy 
(same word with multiple meanings). 
 A raw term-document matrix treats each word independently, which misses semantic 
relationships. 
 LSI reduces the matrix dimensions to capture hidden (latent) semantic structures in text. 
Process of LSI 
1. Construct a TF-IDF matrix from the dataset. 
2. Apply Truncated SVD to decompose into lower-rank matrices. 
3. Represent documents and terms in this reduced semantic space. 
4. Use cosine similarity or other metrics to find similarity between documents/queries. Singular 
Value Decomposition (SVD)

- SVD is a mathematical technique that breaks a large matrix into three smaller matrices. 
- For a document-term matrix (like TF-IDF), SVD helps find patterns/relationships 
between terms and documents. 
- It identifies the most important concepts (latent topics) in the data

PROGRAM
from sklearn.datasets import fetch_20newsgroups 
from sklearn.feature_extraction.text import TfidfVectorizer  
from sklearn.decomposition import TruncatedSVD 
from sklearn.metrics.pairwise import cosine_similarity  
import numpy as np 

# Step 1: Load dataset (subset for speed) 

categories = ['sci.space', 'rec.sport.hockey', 'comp.graphics'] 
newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes')) 

# Step 2: TF-IDF Vectorization 

vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  
X_tfidf = vectorizer.fit_transform(newsgroups.data) 
print(f"Original TF-IDF shape: {X_tfidf.shape}") # (docs x terms) 

# Step 3: SVD for LSI (Latent Semantic Indexing) 

k = 100 # number of latent dimensions  
svd = TruncatedSVD(n_components=k)  
X_lsi = svd.fit_transform(X_tfidf) 
print(f"Reduced LSI shape: {X_lsi.shape}") # (docs x k topics) 

# Step 4: Show similarity between some documents 

def show_similar_docs(query_idx, top_n=5):
    similarities = cosine_similarity([X_lsi[query_idx]], X_lsi)[0]  
    top_indices = similarities.argsort()[::-1][1:top_n+1] 
    print(f"\nQuery Document {query_idx}:\n{newsgroups.data[query_idx][:300]}...\n")  
    print("Top similar documents:") 
    for i in top_indices: 
        print(f"\nDoc #{i} (Similarity: {similarities[i]:.3f}):\n{newsgroups.data[i][:300]}...") 

# Example: Show top 5 similar documents to doc #0 

show_similar_docs(query_idx=0, top_n=5) 

Original TF-IDF shape: (1777, 1000)
Reduced LSI shape: (1777, 100)

Query Document 0:

Mike Vernon is now 3 wins 11 losses plus that All-Star game debacle in
afternoon games during his career...with another afternoon game with
Los Angeles next Sunday...has the ABC deal doomed the Flames?...

Top similar documents:

Doc #342 (Similarity: 0.673):
Dale Hunter ties the game, scoring his third goal of the game
with 2.7 seconds remaining in regulation.

     You could feel it coming on.

     "Due to contractual agreements, ESPN will be unable to carry
the rest of this game live, so that we may show you a worthless
early-season battle between th...

Doc #418 (Similarity: 0.614):
Was the ABC coverage of the Kings/Flames game supposed to be the
way it was shown in BC with CBC overriding the ABC coverage?  When I flipped
to ABC, it was the same commentators, same commercials even.  My question
is:  Was this the real ABC coverage or did CBC just "black out" the 
ABC coverage fo...

Doc #1208 (Similarity: 0.600):
Showing a meaningless (relatively) baseball game over the overtime of
game that was tied up with less than 3 seconds left on the clock?
Gimme a break!  Where does ESPN get these BRILLIANT decisions from?...

Doc #1206 (Similarity: 0.596):
Jesus Christ!!!

The score is now 6-0. The Pens are beating the shit out of the Devils who
gave up in the middle of the 2nd period. ESPN does something smart. The
announcer states "well folks this game is getting out of hand. Lets go to
the Islander/Capitals game." I celebrate as I was actually maki...

Doc #279 (Similarity: 0.591):
As the subject suggests the Flames were not impressive this afternoon,
dropping a 6-3 decision to the LA Kings. Most of the Flames neglected
to show up, especially in their own zone, as the Kings hit at least
five posts! The Flames best line was probably
Skrudland-Paslawski-Berube (which tells how b...

